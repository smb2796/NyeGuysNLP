{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from gensim.models import Word2Vec, word2vec\nimport logging\nimport matplotlib.pyplot as plt\nimport nltk\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\nfrom sklearn.pipeline import Pipeline\nimport sqlite3\nimport re\nfrom tqdm import tqdm"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "logging.basicConfig(level=logging.INFO)\nget_ipython().magic('matplotlib inline')"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "LookupError", 
                    "evalue": "\n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/dsxuser/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-3-d65bba0f5a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/english.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;32m/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'tokenizers/punkt/PY3/english.pickle' not found.\n  Please use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - '/home/dsxuser/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "NLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> l\n\nPackages:\n  [ ] abc................. Australian Broadcasting Commission 2006\n  [ ] alpino.............. Alpino Dutch Treebank\n  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n  [ ] basque_grammars..... Grammars for Basque\n  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n                           Extraction Systems in Biology)\n  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n  [ ] book_grammars....... Grammars from NLTK Book\n  [ ] brown............... Brown Corpus\n  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n  [ ] cess_cat............ CESS-CAT Treebank\n  [ ] cess_esp............ CESS-ESP Treebank\n  [ ] chat80.............. Chat-80 Data Files\n  [ ] city_database....... City Database\n  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n  [ ] comparative_sentences Comparative Sentence Dataset\n  [ ] comtrans............ ComTrans Corpus Sample\n  [ ] conll2000........... CONLL 2000 Chunking Corpus\n  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\nHit Enter to continue: \n  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n                           and Basque Subset)\n  [ ] crubadan............ Crubadan Corpus\n  [ ] dependency_treebank. Dependency Parsed Treebank\n  [ ] dolch............... Dolch Word List\n  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n                           Corpus\n  [ ] floresta............ Portuguese Treebank\n  [ ] framenet_v15........ FrameNet 1.5\n  [ ] framenet_v17........ FrameNet 1.7\n  [ ] gazetteers.......... Gazeteer Lists\n  [ ] genesis............. Genesis Corpus\n  [ ] gutenberg........... Project Gutenberg Selections\n  [ ] ieer................ NIST IE-ER DATA SAMPLE\n  [ ] inaugural........... C-Span Inaugural Address Corpus\n  [ ] indian.............. Indian Language POS-Tagged Corpus\n  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n                           ChaSen format)\n  [ ] kimmo............... PC-KIMMO Data Files\n  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n  [ ] large_grammars...... Large context-free and feature-based grammars\n                           for parser comparison\nHit Enter to continue: \n  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n                           part-of-speech tags\n  [ ] machado............. Machado de Assis -- Obra Completa\n  [ ] masc_tagged......... MASC Tagged Corpus\n  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n  [ ] moses_sample........ Moses Sample Models\n  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n                           2015) subset of the Paraphrase Database.\n  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n  [ ] nombank.1.0......... NomBank Corpus 1.0\n  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n  [ ] nps_chat............ NPS Chat\n  [ ] omw................. Open Multilingual Wordnet\n  [ ] opinion_lexicon..... Opinion Lexicon\n  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n  [ ] paradigms........... Paradigm Corpus\n  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n                           Evaluation Shared Task\nHit Enter to continue: \n  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n                           character properties in Perl\n  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n  [ ] pl196x.............. Polish language of the XX century sixties\n  [ ] porter_test......... Porter Stemmer Test Files\n  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n  [ ] problem_reports..... Problem Report Corpus\n  [ ] product_reviews_1... Product Reviews (5 Products)\n  [ ] product_reviews_2... Product Reviews (9 Products)\n  [ ] propbank............ Proposition Bank Corpus 1.0\n  [ ] pros_cons........... Pros and Cons\n  [ ] ptb................. Penn Treebank\n  [ ] punkt............... Punkt Tokenizer Models\n  [ ] qc.................. Experimental Data for Question Classification\n  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n                           version\n  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n                           Portuguesa)\n  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n  [ ] sample_grammars..... Sample Grammars\n  [ ] semcor.............. SemCor 3.0\nHit Enter to continue: punkt\n  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n  [ ] sentiwordnet........ SentiWordNet\n  [ ] shakespeare......... Shakespeare XML Corpus Sample\n  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n  [ ] smultron............ SMULTRON Corpus Sample\n  [ ] snowball_data....... Snowball Data\n  [ ] spanish_grammars.... Grammars for Spanish\n  [ ] state_union......... C-Span State of the Union Address Corpus\n  [ ] stopwords........... Stopwords Corpus\n  [ ] subjectivity........ Subjectivity Dataset v1.0\n  [ ] swadesh............. Swadesh Wordlists\n  [ ] switchboard......... Switchboard Corpus Sample\n  [ ] tagsets............. Help on Tagsets\n  [ ] timit............... TIMIT Corpus Sample\n  [ ] toolbox............. Toolbox Sample Files\n  [ ] treebank............ Penn Treebank Sample\n  [ ] twitter_samples..... Twitter Samples\n  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n                           (Unicode Version)\n  [ ] udhr................ Universal Declaration of Human Rights Corpus\nHit Enter to continue: d\n  [ ] unicode_samples..... Unicode Samples\n  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n  [ ] vader_lexicon....... VADER Sentiment Lexicon\n  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n  [ ] webtext............. Web Text Corpus\n  [ ] wmt15_eval.......... Evaluation data from WMT15\n  [ ] word2vec_sample..... Word2Vec Sample\n  [ ] wordnet............. WordNet\n  [ ] wordnet_ic.......... WordNet-InfoContent\n  [ ] words............... Word Lists\n  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n                           English Prose\n\nCollections:\n  [ ] all-corpora......... All the corpora\n  [ ] all-nltk............ All packages available on nltk_data gh-pages\n                           branch\n  [ ] all................. All packages\n  [ ] book................ Everything used in the NLTK Book\n  [ ] popular............. Popular packages\nHit Enter to continue: all\n  [ ] tests............... Packages for running tests\n  [ ] third-party......... Third-party data packages\n\n([*] marks installed packages)\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> punkt\nCommand 'punkt' unrecognized\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> punkt\n    Downloading package punkt to /home/dsxuser/nltk_data...\n      Unzipping tokenizers/punkt.zip.\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> x\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> q\n"
                }, 
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "True"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "nltk.download()"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "INFO:ibm_botocore.vendored.requests.packages.urllib3.connectionpool:Starting new HTTPS connection (1): s3-api.us-geo.objectstorage.service.networklayer.com\n"
                }, 
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>B000LQOCH0</td>\n      <td>ABXLMWJIXXAIN</td>\n      <td>Natalia Corres \"Natalia Corres\"</td>\n      <td>1</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1219017600</td>\n      <td>\"Delight\" says it all</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>B000UA0QIQ</td>\n      <td>A395BORC6FGVXV</td>\n      <td>Karl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>2</td>\n      <td>1307923200</td>\n      <td>Cough Medicine</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>B006K2ZZ7K</td>\n      <td>A1UQRSCLF8GW1T</td>\n      <td>Michael D. Bigham \"M. Wassir\"</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1350777600</td>\n      <td>Great taffy</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   Id   ProductId          UserId                      ProfileName  \\\n0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n\n   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n0                     1                       1      5  1303862400   \n1                     0                       0      1  1346976000   \n2                     1                       1      4  1219017600   \n3                     3                       3      2  1307923200   \n4                     0                       0      5  1350777600   \n\n                 Summary                                               Text  \n0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n2  \"Delight\" says it all  This is a confection that has been around a fe...  \n3         Cough Medicine  If you are looking for the secret ingredient i...  \n4            Great taffy  Great taffy at a great price.  There was a wid...  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reviews = reviews[reviews.Score != 3]"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f79f265c5f8>]], dtype=object)"
                    }, 
                    "output_type": "execute_result"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "image/png": "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\n", 
                        "text/plain": "<matplotlib.figure.Figure at 0x7f79f265c358>"
                    }, 
                    "metadata": {}
                }
            ], 
            "source": "reviews.hist('Score')"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reviews['Class'] = 1 * (reviews['Score'] > 3)"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>ProductId</th>\n      <th>UserId</th>\n      <th>ProfileName</th>\n      <th>HelpfulnessNumerator</th>\n      <th>HelpfulnessDenominator</th>\n      <th>Score</th>\n      <th>Time</th>\n      <th>Summary</th>\n      <th>Text</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>B001E4KFG0</td>\n      <td>A3SGXH7AUHU8GW</td>\n      <td>delmartian</td>\n      <td>1</td>\n      <td>1</td>\n      <td>5</td>\n      <td>1303862400</td>\n      <td>Good Quality Dog Food</td>\n      <td>I have bought several of the Vitality canned d...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>B00813GRG4</td>\n      <td>A1D87F6ZCVE5NK</td>\n      <td>dll pa</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1346976000</td>\n      <td>Not as Advertised</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "   Id   ProductId          UserId ProfileName  HelpfulnessNumerator  \\\n0   1  B001E4KFG0  A3SGXH7AUHU8GW  delmartian                     1   \n1   2  B00813GRG4  A1D87F6ZCVE5NK      dll pa                     0   \n\n   HelpfulnessDenominator  Score        Time                Summary  \\\n0                       1      5  1303862400  Good Quality Dog Food   \n1                       0      1  1346976000      Not as Advertised   \n\n                                                Text  Class  \n0  I have bought several of the Vitality canned d...      1  \n1  Product arrived labeled as Jumbo Salted Peanut...      0  "
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "reviews.head(n=2)"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "reviews.sort_values('ProductId', axis=0, inplace=True)"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train_size = int(len(reviews) * 0.5)\ntrain_reviews = reviews.iloc[:train_size,:]\ntest_reviews = reviews.iloc[train_size:,:]"
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "test_remove = np.logical_or(test_reviews['ProductId'].isin(train_reviews['ProductId']),\n                          test_reviews['UserId'].isin(train_reviews['UserId']))\ntest_reviews = test_reviews[np.logical_not(test_remove)]"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training set contains 262907 reviews.\nTest set contains 151198 reviews (111709 removed).\n"
                }
            ], 
            "source": "print('Training set contains {:d} reviews.'.format(len(train_reviews)))\nprint('Test set contains {:d} reviews ({:d} removed).'.format(len(test_reviews), sum(test_remove)))"
        }, 
        {
            "execution_count": 14, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training set contains 84.93% positive reviews\nTest set contains 82.37% positive reviews\n"
                }
            ], 
            "source": "n_pos_train = sum(train_reviews['Class'] == 1)\nprint('Training set contains {:.2%} positive reviews'.format(n_pos_train/len(train_reviews)))\nn_pos_test = sum(test_reviews['Class'] == 1)\nprint('Test set contains {:.2%} positive reviews'.format(n_pos_test/len(test_reviews)))"
        }, 
        {
            "execution_count": 15, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "del reviews"
        }, 
        {
            "execution_count": 16, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def review_to_wordlist(review, remove_stopwords=False):\n    \"\"\"\n    Convert a review to a list of words. Removal of stop words is optional.\n    \"\"\"\n    # remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review)\n    \n    # convert to lower case and split at whitespace\n    words = review_text.lower().split()\n    \n    # remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n\n    return words"
        }, 
        {
            "execution_count": 17, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n    \"\"\"\n    Split review into list of sentences where each sentence is a list of words.\n    Removal of stop words is optional.\n    \"\"\"\n    # use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n\n    # each sentence is furthermore split into words\n    sentences = []    \n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n            \n    return sentences"
        }, 
        {
            "execution_count": 18, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "NameError", 
                    "evalue": "name 'tokenizer' is not defined", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-18-0f572486e876>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Initialize an empty list of sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_sentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreview_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "train_sentences = []  # Initialize an empty list of sentences\nfor review in train_reviews['Text']:\n    train_sentences += review_to_sentences(review, tokenizer)"
        }, 
        {
            "execution_count": 19, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "ename": "IndexError", 
                    "evalue": "list index out of range", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-19-ffc44fef271b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;31mIndexError\u001b[0m: list index out of range"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "train_sentences[0]"
        }, 
        {
            "execution_count": 20, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
        }, 
        {
            "execution_count": 21, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "train_sentences = []  # Initialize an empty list of sentences\nfor review in train_reviews['Text']:\n    train_sentences += review_to_sentences(review, tokenizer)"
        }, 
        {
            "execution_count": 22, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 22, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "['this',\n 'witty',\n 'little',\n 'book',\n 'makes',\n 'my',\n 'son',\n 'laugh',\n 'at',\n 'loud']"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "train_sentences[0]"
        }, 
        {
            "execution_count": 23, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "INFO:gensim.models.word2vec:collecting all words and their counts\nINFO:gensim.models.word2vec:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #10000, processed 180428 words, keeping 8069 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #20000, processed 359188 words, keeping 11891 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #30000, processed 524070 words, keeping 15515 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #40000, processed 680380 words, keeping 17879 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #50000, processed 842552 words, keeping 20707 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #60000, processed 1004577 words, keeping 22649 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #70000, processed 1167004 words, keeping 24754 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #80000, processed 1320862 words, keeping 25385 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #90000, processed 1490399 words, keeping 26728 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #100000, processed 1652964 words, keeping 27605 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #110000, processed 1820823 words, keeping 28651 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #120000, processed 1995726 words, keeping 30232 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #130000, processed 2162244 words, keeping 31400 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #140000, processed 2330605 words, keeping 32324 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #150000, processed 2497196 words, keeping 33059 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #160000, processed 2657117 words, keeping 33871 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #170000, processed 2816870 words, keeping 34759 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #180000, processed 2981871 words, keeping 35634 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #190000, processed 3159640 words, keeping 36318 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #200000, processed 3328868 words, keeping 37267 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #210000, processed 3487336 words, keeping 37910 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #220000, processed 3645618 words, keeping 38757 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #230000, processed 3804465 words, keeping 39479 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #240000, processed 3965130 words, keeping 39655 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #250000, processed 4124553 words, keeping 39661 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #260000, processed 4284220 words, keeping 40104 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #270000, processed 4442962 words, keeping 40692 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #280000, processed 4599727 words, keeping 41115 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #290000, processed 4760344 words, keeping 41816 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #300000, processed 4922629 words, keeping 42359 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #310000, processed 5079635 words, keeping 42906 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #320000, processed 5237498 words, keeping 43677 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #330000, processed 5394753 words, keeping 44185 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #340000, processed 5552023 words, keeping 44461 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #350000, processed 5703169 words, keeping 44888 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #360000, processed 5853108 words, keeping 45369 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #370000, processed 6010727 words, keeping 46014 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #380000, processed 6172789 words, keeping 46661 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #390000, processed 6337696 words, keeping 47259 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #400000, processed 6494452 words, keeping 47666 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #410000, processed 6652355 words, keeping 48283 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #420000, processed 6815199 words, keeping 48806 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #430000, processed 6971053 words, keeping 49371 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #440000, processed 7125020 words, keeping 49626 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #450000, processed 7279040 words, keeping 50019 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #460000, processed 7431507 words, keeping 50333 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #470000, processed 7587748 words, keeping 50771 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #480000, processed 7747819 words, keeping 51246 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #490000, processed 7909136 words, keeping 51499 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #500000, processed 8067395 words, keeping 51968 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #510000, processed 8223527 words, keeping 52403 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #520000, processed 8378958 words, keeping 52829 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #530000, processed 8537773 words, keeping 53394 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #540000, processed 8690398 words, keeping 53779 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #550000, processed 8842053 words, keeping 54215 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #560000, processed 9004279 words, keeping 54913 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #570000, processed 9164023 words, keeping 55335 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #580000, processed 9330192 words, keeping 55887 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #590000, processed 9494137 words, keeping 56239 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #600000, processed 9652921 words, keeping 56548 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #610000, processed 9809724 words, keeping 56887 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #620000, processed 9969697 words, keeping 57308 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #630000, processed 10131795 words, keeping 57999 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #640000, processed 10295238 words, keeping 58537 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #650000, processed 10455800 words, keeping 59002 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #660000, processed 10612501 words, keeping 59491 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #670000, processed 10767283 words, keeping 60005 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #680000, processed 10933050 words, keeping 60534 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #690000, processed 11091575 words, keeping 60917 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #700000, processed 11251827 words, keeping 61285 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #710000, processed 11412990 words, keeping 61519 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #720000, processed 11568608 words, keeping 61975 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #730000, processed 11729893 words, keeping 62473 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #740000, processed 11888227 words, keeping 62950 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #750000, processed 12045035 words, keeping 63372 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #760000, processed 12207760 words, keeping 63731 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #770000, processed 12367459 words, keeping 64261 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #780000, processed 12540406 words, keeping 64548 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #790000, processed 12714651 words, keeping 64963 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #800000, processed 12909315 words, keeping 65190 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #810000, processed 13102866 words, keeping 65307 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #820000, processed 13265784 words, keeping 65637 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #830000, processed 13425109 words, keeping 66033 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #840000, processed 13583257 words, keeping 66253 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #850000, processed 13750179 words, keeping 66673 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #860000, processed 13912689 words, keeping 66960 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #870000, processed 14080276 words, keeping 67276 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #880000, processed 14253592 words, keeping 67585 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #890000, processed 14425242 words, keeping 67695 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #900000, processed 14584406 words, keeping 67902 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #910000, processed 14752165 words, keeping 68090 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #920000, processed 14922356 words, keeping 68452 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #930000, processed 15078725 words, keeping 68823 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #940000, processed 15234489 words, keeping 69234 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #950000, processed 15397711 words, keeping 69630 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #960000, processed 15555781 words, keeping 70069 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #970000, processed 15737605 words, keeping 70296 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #980000, processed 15895638 words, keeping 70505 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #990000, processed 16058462 words, keeping 70874 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1000000, processed 16219005 words, keeping 71013 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1010000, processed 16383036 words, keeping 71400 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1020000, processed 16577581 words, keeping 71604 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1030000, processed 16747920 words, keeping 71777 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1040000, processed 16918066 words, keeping 71942 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1050000, processed 17076861 words, keeping 72257 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1060000, processed 17229888 words, keeping 72395 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1070000, processed 17390441 words, keeping 72661 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1080000, processed 17558493 words, keeping 72866 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1090000, processed 17715910 words, keeping 73246 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1100000, processed 17875115 words, keeping 73599 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1110000, processed 18040043 words, keeping 73952 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1120000, processed 18198519 words, keeping 74277 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1130000, processed 18360987 words, keeping 74600 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1140000, processed 18521917 words, keeping 74963 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1150000, processed 18684802 words, keeping 75226 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1160000, processed 18847163 words, keeping 75541 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1170000, processed 19005099 words, keeping 75888 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1180000, processed 19168066 words, keeping 76181 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1190000, processed 19330830 words, keeping 76627 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1200000, processed 19489307 words, keeping 76865 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1210000, processed 19651401 words, keeping 77162 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1220000, processed 19809331 words, keeping 77545 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1230000, processed 19969411 words, keeping 77954 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1240000, processed 20128485 words, keeping 78238 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1250000, processed 20286688 words, keeping 78447 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1260000, processed 20450422 words, keeping 78777 word types\nINFO:gensim.models.word2vec:PROGRESS: at sentence #1270000, processed 20615492 words, keeping 79114 word types\nINFO:gensim.models.word2vec:collected 79173 word types from a corpus of 20644316 raw words and 1271906 sentences\nINFO:gensim.models.word2vec:Loading a fresh vocabulary\nINFO:gensim.models.word2vec:effective_min_count=40 retains 10821 unique words (13% of original 79173, drops 68352)\nINFO:gensim.models.word2vec:effective_min_count=40 leaves 20305171 word corpus (98% of original 20644316, drops 339145)\nINFO:gensim.models.word2vec:deleting the raw counts dictionary of 79173 items\nINFO:gensim.models.word2vec:sample=0.001 downsamples 55 most-common words\nINFO:gensim.models.word2vec:downsampling leaves estimated 14845841 word corpus (73.1% of prior 20305171)\nINFO:gensim.models.base_any2vec:estimated required memory for 10821 words and 300 dimensions: 31380900 bytes\nINFO:gensim.models.word2vec:resetting layer weights\nINFO:gensim.models.base_any2vec:training model with 3 workers on 10821 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 2.43% examples, 380541 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 5.17% examples, 388183 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 7.84% examples, 389217 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 10.32% examples, 384637 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 12.99% examples, 386746 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 15.51% examples, 387805 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 18.23% examples, 388401 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 20.99% examples, 389576 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 23.64% examples, 389335 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 26.40% examples, 389584 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 29.24% examples, 390679 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 31.95% examples, 390640 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 34.73% examples, 391117 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 37.56% examples, 390569 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 40.33% examples, 391004 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 43.23% examples, 391669 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 45.90% examples, 391930 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 48.69% examples, 391493 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 51.40% examples, 391864 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 54.18% examples, 391832 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 56.99% examples, 392340 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 59.79% examples, 392869 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 62.33% examples, 392891 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 64.65% examples, 392313 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 67.37% examples, 392352 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 69.91% examples, 392267 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 72.57% examples, 391936 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 75.30% examples, 391880 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 77.80% examples, 391574 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 80.24% examples, 391483 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 82.86% examples, 391597 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 85.60% examples, 391915 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 88.25% examples, 391721 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 90.95% examples, 391870 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 93.69% examples, 391986 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 96.49% examples, 391828 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 1 - PROGRESS: at 99.21% examples, 391977 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\nINFO:gensim.models.base_any2vec:EPOCH - 1 : training on 20644316 raw words (14844454 effective words) took 37.8s, 392340 effective words/s\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 3.04% examples, 459420 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 6.37% examples, 469838 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 8.90% examples, 443276 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 12.08% examples, 454619 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 15.24% examples, 459539 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 18.47% examples, 461637 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 21.40% examples, 456047 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 24.04% examples, 447378 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 26.90% examples, 443217 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 29.63% examples, 437577 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 32.33% examples, 433357 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 35.09% examples, 430105 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 37.96% examples, 427749 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 40.73% examples, 426048 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 43.95% examples, 427976 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 46.66% examples, 425126 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 49.46% examples, 423631 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 52.28% examples, 422655 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 54.96% examples, 420972 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 57.71% examples, 420001 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 60.48% examples, 418961 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 62.80% examples, 417458 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 65.30% examples, 416664 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 68.00% examples, 415899 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 70.52% examples, 414651 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 73.18% examples, 413518 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 75.87% examples, 412825 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 78.58% examples, 412176 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 80.99% examples, 410940 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 83.74% examples, 410567 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 86.55% examples, 410485 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 89.27% examples, 410092 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 91.99% examples, 409688 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 94.63% examples, 408879 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 2 - PROGRESS: at 97.52% examples, 408610 words/s, in_qsize 4, out_qsize 1\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\nINFO:gensim.models.base_any2vec:EPOCH - 2 : training on 20644316 raw words (14847321 effective words) took 36.2s, 410480 effective words/s\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 2.49% examples, 375823 words/s, in_qsize 4, out_qsize 1\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 5.27% examples, 389214 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 7.89% examples, 388027 words/s, in_qsize 4, out_qsize 1\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 10.46% examples, 391608 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 13.19% examples, 393304 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 15.76% examples, 393973 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 18.47% examples, 393853 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 21.18% examples, 393151 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 23.84% examples, 392400 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 26.54% examples, 391774 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 29.34% examples, 391696 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 32.05% examples, 390629 words/s, in_qsize 6, out_qsize 2\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 34.89% examples, 391860 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 37.65% examples, 391372 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 40.43% examples, 391678 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 43.27% examples, 391127 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 45.95% examples, 391304 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 48.64% examples, 391001 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 51.40% examples, 391675 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 54.07% examples, 391404 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 56.89% examples, 391738 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 59.65% examples, 391368 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 62.19% examples, 391212 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 64.47% examples, 390989 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 67.12% examples, 391125 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 69.68% examples, 391245 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 72.33% examples, 391522 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 75.00% examples, 391324 words/s, in_qsize 6, out_qsize 1\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 77.57% examples, 391252 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 80.04% examples, 390682 words/s, in_qsize 3, out_qsize 2\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 82.61% examples, 390917 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 85.39% examples, 391351 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 88.10% examples, 391312 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 90.76% examples, 391322 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 93.44% examples, 391143 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 96.19% examples, 391421 words/s, in_qsize 4, out_qsize 1\nINFO:gensim.models.base_any2vec:EPOCH 3 - PROGRESS: at 98.95% examples, 391116 words/s, in_qsize 4, out_qsize 1\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\nINFO:gensim.models.base_any2vec:EPOCH - 3 : training on 20644316 raw words (14846728 effective words) took 37.9s, 391663 effective words/s\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 2.29% examples, 354237 words/s, in_qsize 4, out_qsize 1\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 4.98% examples, 371773 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 7.69% examples, 381158 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 10.20% examples, 382835 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 12.83% examples, 384883 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 15.42% examples, 386286 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 18.13% examples, 386641 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 20.90% examples, 388238 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 23.64% examples, 387370 words/s, in_qsize 3, out_qsize 2\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 26.74% examples, 392921 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 29.63% examples, 393607 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 32.86% examples, 399819 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 36.33% examples, 406525 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 39.28% examples, 408063 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 42.71% examples, 413317 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 46.00% examples, 416859 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 49.31% examples, 420429 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 52.72% examples, 424437 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 55.93% examples, 426961 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 59.36% examples, 429919 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 62.45% examples, 432265 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 65.39% examples, 434345 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 68.65% examples, 436352 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 71.39% examples, 435210 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 74.12% examples, 433806 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 76.72% examples, 432237 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 79.34% examples, 430413 words/s, in_qsize 5, out_qsize 1\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 81.77% examples, 429263 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 84.49% examples, 428069 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 87.71% examples, 429365 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 90.95% examples, 430768 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 93.79% examples, 429666 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 4 - PROGRESS: at 96.94% examples, 430343 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\nINFO:gensim.models.base_any2vec:EPOCH - 4 : training on 20644316 raw words (14845008 effective words) took 34.4s, 431945 effective words/s\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 2.59% examples, 405140 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 5.36% examples, 395741 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 8.08% examples, 395398 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 10.65% examples, 396836 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 13.64% examples, 405769 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 16.26% examples, 404698 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 18.91% examples, 401618 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 21.65% examples, 400439 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 24.39% examples, 400222 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 27.20% examples, 400491 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 29.92% examples, 397973 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 32.69% examples, 398000 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 35.39% examples, 397277 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 38.19% examples, 397194 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 40.94% examples, 397148 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 43.76% examples, 397078 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 46.46% examples, 396960 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 49.26% examples, 397383 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 52.03% examples, 396466 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 54.78% examples, 396645 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 57.47% examples, 396295 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 60.24% examples, 396243 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 62.68% examples, 396379 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 65.10% examples, 396190 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 67.77% examples, 395937 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 70.32% examples, 395416 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 72.88% examples, 395075 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 75.63% examples, 395085 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 78.20% examples, 394843 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 80.71% examples, 394903 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 83.33% examples, 394820 words/s, in_qsize 6, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 85.96% examples, 394580 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 88.92% examples, 395761 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 91.99% examples, 396810 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 94.72% examples, 396898 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:EPOCH 5 - PROGRESS: at 97.47% examples, 396883 words/s, in_qsize 5, out_qsize 0\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 2 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 1 more threads\nINFO:gensim.models.base_any2vec:worker thread finished; awaiting finish of 0 more threads\nINFO:gensim.models.base_any2vec:EPOCH - 5 : training on 20644316 raw words (14846288 effective words) took 37.4s, 397310 effective words/s\nINFO:gensim.models.base_any2vec:training on a 103221580 raw words (74229799 effective words) took 183.7s, 404106 effective words/s\nINFO:gensim.models.keyedvectors:precomputing L2-norms of word weight vectors\nINFO:gensim.utils:saving Word2Vec object under train_model, separately None\nINFO:gensim.utils:not storing attribute vectors_norm\nINFO:gensim.utils:not storing attribute cum_table\nINFO:gensim.utils:saved train_model\n"
                }
            ], 
            "source": "model_name = 'train_model'\n# Set values for various word2vec parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 3       # Number of threads to run in parallel\ncontext = 10          # Context window size\ndownsampling = 1e-3   # Downsample setting for frequent words\nif not os.path.exists(model_name): \n    # Initialize and train the model (this will take some time)\n    model = word2vec.Word2Vec(train_sentences, workers=num_workers, \\\n                size=num_features, min_count = min_word_count, \\\n                window = context, sample = downsampling)\n\n    # If you don't plan to train the model any further, calling \n    # init_sims will make the model much more memory-efficient.\n    model.init_sims(replace=True)\n\n    # It can be helpful to create a meaningful model name and \n    # save the model for later use. You can load it later using Word2Vec.load()\n    model.save(model_name)\nelse:\n    model = Word2Vec.load(model_name)"
        }, 
        {
            "execution_count": 24, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "del train_sentences"
        }, 
        {
            "execution_count": 25, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n  if __name__ == '__main__':\n"
                }, 
                {
                    "execution_count": 25, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'sausage'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.doesnt_match(\"banana apple orange sausage\".split())"
        }, 
        {
            "execution_count": 27, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 27, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'sausage'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.wv.doesnt_match(\"banana apple orange sausage\".split())"
        }, 
        {
            "execution_count": 28, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 28, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "'dish'"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.wv.doesnt_match(\"vanilla chocolate cinnamon dish\".split())"
        }, 
        {
            "execution_count": 29, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n  if __name__ == '__main__':\n"
                }, 
                {
                    "execution_count": 29, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('fantastic', 0.7866164445877075),\n ('terrific', 0.7834751605987549),\n ('wonderful', 0.7685626745223999),\n ('good', 0.7085849046707153),\n ('fabulous', 0.7068175673484802),\n ('awesome', 0.670752763748169),\n ('excellent', 0.6457017660140991),\n ('superb', 0.6306446194648743),\n ('perfect', 0.6197716593742371),\n ('amazing', 0.6015411615371704)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.most_similar(\"great\")"
        }, 
        {
            "execution_count": 30, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 30, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('terrible', 0.7386919856071472),\n ('horrible', 0.7337364554405212),\n ('disgusting', 0.6804637908935547),\n ('gross', 0.6639425754547119),\n ('aweful', 0.6588665246963501),\n ('nasty', 0.602871835231781),\n ('horrid', 0.6009899377822876),\n ('amazing', 0.5724624395370483),\n ('dreadful', 0.5691972970962524),\n ('awesome', 0.5685483813285828)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.wv.most_similar(\"awful\")"
        }, 
        {
            "execution_count": 31, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 31, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('charles', 0.5279216766357422),\n ('cavalier', 0.49235987663269043),\n ('spaniel', 0.4289575219154358),\n ('cocker', 0.4020504355430603),\n ('springer', 0.40188121795654297),\n ('arthur', 0.39694249629974365),\n ('breeder', 0.3966832756996155),\n ('md', 0.3817521035671234),\n ('jake', 0.37769854068756104),\n ('spaniels', 0.37650030851364136)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
        }, 
        {
            "execution_count": 32, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n  if __name__ == '__main__':\n"
                }, 
                {
                    "execution_count": 32, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[('beer', 0.6428319215774536),\n ('sushi', 0.312652051448822),\n ('diner', 0.30898433923721313),\n ('style', 0.3053444027900696),\n ('enchiladas', 0.3039426803588867),\n ('grandmother', 0.29402580857276917),\n ('burritos', 0.2922184467315674),\n ('grandma', 0.28205499053001404),\n ('boyardee', 0.28004831075668335),\n ('tortillas', 0.27907007932662964)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.wv.similar_by_vector(model['beer'] - model['alcohol'])"
        }, 
        {
            "execution_count": 34, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n  if __name__ == '__main__':\n"
                }, 
                {
                    "execution_count": 34, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(10821, 300)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "model.wv.syn0.shape"
        }, 
        {
            "execution_count": 42, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "def make_feature_vec(words, model, num_features):\n    \"\"\"\n    Average the word vectors for a set of words\n    \"\"\"\n    feature_vec = np.zeros((num_features,),dtype=\"float32\")  # pre-initialize (for speed)\n    nwords = 0.\n    index2word_set = set(model.wv.index2word)  # words known to the model\n\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            feature_vec = np.add(feature_vec,model[word])\n    \n    feature_vec = np.divide(feature_vec, nwords)\n    return feature_vec\n\n\ndef get_avg_feature_vecs(reviews, model, num_features):\n    \"\"\"\n    Calculate average feature vectors for all reviews\n    \"\"\"\n    counter = 0.\n    review_feature_vecs = np.zeros((len(reviews),num_features), dtype='float32')  # pre-initialize (for speed)\n    \n    for review in reviews:\n        review_feature_vecs[counter] = make_feature_vec(review, model, num_features)\n        counter = counter + 1.\n    return review_feature_vecs"
        }, 
        {
            "execution_count": 43, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stderr", 
                    "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
                }, 
                {
                    "ename": "IndexError", 
                    "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-43-18704b359091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_avg_feature_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m", 
                        "\u001b[0;32m<ipython-input-42-d853bf97966d>\u001b[0m in \u001b[0;36mget_avg_feature_vecs\u001b[0;34m(reviews, model, num_features)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mreview_feature_vecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_feature_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreview_feature_vecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "# calculate average feature vectors for training and test sets\nclean_train_reviews = []\nfor review in train_reviews['Text']:\n    clean_train_reviews.append(review_to_wordlist(review, remove_stopwords=True))\ntrainDataVecs = get_avg_feature_vecs(clean_train_reviews, model, num_features)"
        }, 
        {
            "execution_count": 37, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "NLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> corpora\n    Error loading corpora: Package 'corpora' not found in index\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> l\n\nPackages:\n  [ ] abc................. Australian Broadcasting Commission 2006\n  [ ] alpino.............. Alpino Dutch Treebank\n  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n  [ ] basque_grammars..... Grammars for Basque\n  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n                           Extraction Systems in Biology)\n  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n  [ ] book_grammars....... Grammars from NLTK Book\n  [ ] brown............... Brown Corpus\n  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n  [ ] cess_cat............ CESS-CAT Treebank\n  [ ] cess_esp............ CESS-ESP Treebank\n  [ ] chat80.............. Chat-80 Data Files\n  [ ] city_database....... City Database\n  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n  [ ] comparative_sentences Comparative Sentence Dataset\n  [ ] comtrans............ ComTrans Corpus Sample\n  [ ] conll2000........... CONLL 2000 Chunking Corpus\n  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\nHit Enter to continue: \n  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n                           and Basque Subset)\n  [ ] crubadan............ Crubadan Corpus\n  [ ] dependency_treebank. Dependency Parsed Treebank\n  [ ] dolch............... Dolch Word List\n  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n                           Corpus\n  [ ] floresta............ Portuguese Treebank\n  [ ] framenet_v15........ FrameNet 1.5\n  [ ] framenet_v17........ FrameNet 1.7\n  [ ] gazetteers.......... Gazeteer Lists\n  [ ] genesis............. Genesis Corpus\n  [ ] gutenberg........... Project Gutenberg Selections\n  [ ] ieer................ NIST IE-ER DATA SAMPLE\n  [ ] inaugural........... C-Span Inaugural Address Corpus\n  [ ] indian.............. Indian Language POS-Tagged Corpus\n  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n                           ChaSen format)\n  [ ] kimmo............... PC-KIMMO Data Files\n  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n  [ ] large_grammars...... Large context-free and feature-based grammars\n                           for parser comparison\nHit Enter to continue: \n  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n                           part-of-speech tags\n  [ ] machado............. Machado de Assis -- Obra Completa\n  [ ] masc_tagged......... MASC Tagged Corpus\n  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n  [ ] moses_sample........ Moses Sample Models\n  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n                           2015) subset of the Paraphrase Database.\n  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n  [ ] nombank.1.0......... NomBank Corpus 1.0\n  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n  [ ] nps_chat............ NPS Chat\n  [ ] omw................. Open Multilingual Wordnet\n  [ ] opinion_lexicon..... Opinion Lexicon\n  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n  [ ] paradigms........... Paradigm Corpus\n  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n                           Evaluation Shared Task\nHit Enter to continue: \n  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n                           character properties in Perl\n  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n  [ ] pl196x.............. Polish language of the XX century sixties\n  [ ] porter_test......... Porter Stemmer Test Files\n  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n  [ ] problem_reports..... Problem Report Corpus\n  [ ] product_reviews_1... Product Reviews (5 Products)\n  [ ] product_reviews_2... Product Reviews (9 Products)\n  [ ] propbank............ Proposition Bank Corpus 1.0\n  [ ] pros_cons........... Pros and Cons\n  [ ] ptb................. Penn Treebank\n  [*] punkt............... Punkt Tokenizer Models\n  [ ] qc.................. Experimental Data for Question Classification\n  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n                           version\n  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n                           Portuguesa)\n  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n  [ ] sample_grammars..... Sample Grammars\n  [ ] semcor.............. SemCor 3.0\nHit Enter to continue: \n  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n  [ ] sentiwordnet........ SentiWordNet\n  [ ] shakespeare......... Shakespeare XML Corpus Sample\n  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n  [ ] smultron............ SMULTRON Corpus Sample\n  [ ] snowball_data....... Snowball Data\n  [ ] spanish_grammars.... Grammars for Spanish\n  [ ] state_union......... C-Span State of the Union Address Corpus\n  [ ] stopwords........... Stopwords Corpus\n  [ ] subjectivity........ Subjectivity Dataset v1.0\n  [ ] swadesh............. Swadesh Wordlists\n  [ ] switchboard......... Switchboard Corpus Sample\n  [ ] tagsets............. Help on Tagsets\n  [ ] timit............... TIMIT Corpus Sample\n  [ ] toolbox............. Toolbox Sample Files\n  [ ] treebank............ Penn Treebank Sample\n  [ ] twitter_samples..... Twitter Samples\n  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n                           (Unicode Version)\n  [ ] udhr................ Universal Declaration of Human Rights Corpus\nHit Enter to continue: d\n  [ ] unicode_samples..... Unicode Samples\n  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n  [ ] vader_lexicon....... VADER Sentiment Lexicon\n  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n  [ ] webtext............. Web Text Corpus\n  [ ] wmt15_eval.......... Evaluation data from WMT15\n  [ ] word2vec_sample..... Word2Vec Sample\n  [ ] wordnet............. WordNet\n  [ ] wordnet_ic.......... WordNet-InfoContent\n  [ ] words............... Word Lists\n  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n                           English Prose\n\nCollections:\n  [ ] all-corpora......... All the corpora\n  [P] all-nltk............ All packages available on nltk_data gh-pages\n                           branch\n  [P] all................. All packages\n  [P] book................ Everything used in the NLTK Book\n  [P] popular............. Popular packages\nHit Enter to continue: stopwords\n  [ ] tests............... Packages for running tests\n  [ ] third-party......... Third-party data packages\n\n([*] marks installed packages; [P] marks partially installed collections)\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> d\n\nDownload which package (l=list; x=cancel)?\n  Identifier> stopwords\n    Downloading package stopwords to /home/dsxuser/nltk_data...\n      Unzipping corpora/stopwords.zip.\n\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> q\n"
                }, 
                {
                    "execution_count": 37, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "True"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "nltk.download()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "clean_test_reviews = []\nfor review in test_reviews['Text']:\n    clean_test_reviews.append(review_to_wordlist(review, remove_stopwords=True))\ntestDataVecs = get_avg_feature_vecs(clean_test_reviews, model, num_features)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Fit a random forest to the training data, using 100 trees\nforest = RandomForestClassifier(n_estimators = 100)\n\nprint(\"Fitting a random forest to labeled training data...\")\nforest = forest.fit(trainDataVecs, train_reviews['Class'])"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# remove instances in test set that could not be represented as feature vectors\nnan_indices = list({x for x,y in np.argwhere(np.isnan(testDataVecs))})\nif len(nan_indices) > 0:\n    print('Removing {:d} instances from test set.'.format(len(nan_indices)))\n    testDataVecs = np.delete(testDataVecs, nan_indices, axis=0)\n    test_reviews.drop(test_reviews.iloc[nan_indices, :].index, axis=0, inplace=True)\n    assert testDataVecs.shape[0] == len(test_reviews)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(\"Predicting labels for test data..\")\nresult = forest.predict(testDataVecs)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "print(classification_report(test_reviews['Class'], result))"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }
    }, 
    "nbformat": 4
}